<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>In principio erat Verbum - Machine Learning</title>
    <link rel="stylesheet" href="/theme/css/main.css" />

    <!--[if IE]>
    <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>

  <body id="index" class="home">
    <header id="banner" class="body">
      <h1><a href="/">In principio erat Verbum </a></h1>
      <nav><ul>
        <li><a href="/category/expository.html">Expository</a></li>
        <li><a href="/category/math-212.html">Math 212</a></li>
      </ul></nav>
    </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/scala_graphx.html">Scala, Spark, and Graphx</a></h1>
<footer class="post-info">
        <abbr class="published" title="2015-09-29T17:54:00-04:00">
                Published: Tue 29 September 2015
        </abbr>
		<br />
        <abbr class="modified" title="2015-10-23T00:00:00-04:00">
                Updated: Fri 23 October 2015
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/tingran-gao.html">Tingran Gao</a>
        </address>
<p>In <a href="/category/expository.html">Expository</a>.</p>
<p>tags: <a href="/tag/scala.html">Scala</a> <a href="/tag/machine-learning.html">Machine Learning</a> <a href="/tag/probabilistic-graphical-model.html">Probabilistic Graphical Model</a> </p>
</footer><!-- /.post-info --><div class="toc">
<ul>
<li><a href="#set-up-emacsensime">Set up Emacs+Ensime</a></li>
<li><a href="#compiling-spark-from-source">Compiling Spark from Source</a></li>
<li><a href="#using-graphx-to-build-a-bipartite-graph">Using GraphX to Build a Bipartite Graph</a></li>
</ul>
</div>
<hr />
<h2 id="set-up-emacsensime">Set up Emacs+Ensime</h2>
<p>The basic reference is on the official GitHub page of ensime: <a href="https://github.com/ensime/ensime-emacs">https://github.com/ensime/ensime-emacs</a></p>
<p>First, <code>M-x package-install</code> in Emacs, choose <code>ensime</code>, mark as <code>I</code> and press <code>X</code>. This installs the package <code>ensime</code> for your current Emacs. Then update the <code>.emacs</code> file with</p>
<div class="highlight"><pre>    (require &#39;package)
    (package-initialize)
    (add-to-list &#39;package-archives
                 &#39;(&quot;melpa&quot; . &quot;http://melpa.org/packages/&quot;) t)
    (package-initialize)

    (when (not package-archive-contents)
      (package-refresh-contents))

    ;; ensime for scala mode hook                                                                                                                                               
    (require &#39;ensime)
    (add-hook &#39;scala-mode-hook &#39;ensime-scala-mode-hook)

    ;; OPTIONAL                                                                                                                                                                 
    ;; there are some great Scala yasnippets, browse through:                                                                                                                   
    ;; https://github.com/AndreaCrotti/yasnippet-snippets/tree/master/scala-mode                                                                                                
    (add-hook &#39;scala-mode-hook #&#39;yas-minor-mode)
    ;; but company-mode / yasnippet conflict. Disable TAB in company-mode with                                                                                                  
    (define-key company-active-map [tab] nil)
</pre></div>


<p>The next step is to choose a project management tool. You can choose among <a href="https://github.com/ensime/ensime-sbt">sbt</a>, <a href="https://github.com/ensime/ensime-gradle">gradle</a>, and <a href="https://github.com/ensime/ensime-maven">maven</a>. For this simple quick test I'm using sbt. If that's also your preference, you have to create file <code>~/.sbt/0.13/plugins/plugins.sbt</code> and write in the following:</p>
<div class="highlight"><pre>    addSbtPlugin(&quot;org.ensime&quot; % &quot;ensime-sbt&quot; % &quot;0.2.0&quot;)
</pre></div>


<p>Now it is time to create your project folder, <code>cd</code> into it, then</p>
<div class="highlight"><pre>    mkdir -p src/{main,test}/scala
</pre></div>


<p>Also in the project folder, create <code>build.sbt</code> with the following content:</p>
<div class="highlight"><pre>    name := &quot;hello_scala&quot;

    version := &quot;1.0&quot;

    scalaVersion := &quot;2.10.5&quot;
</pre></div>


<p>Note that the blank lines are necessary. Now let us generate .ensime file by typing into the terminal</p>
<div class="highlight"><pre>    sbt gen-ensime
</pre></div>


<p>A hidden file <code>.ensime</code> will be added to the project folder, together with a folder <code>project</code>. Create a file under <code>src/main/scala/</code>, say with name <code>Main.scala</code>. Write in it</p>
<div class="highlight"><pre>    package greeter

    object Hello extends App {
      println(&quot;Hello World&quot;)
    }
</pre></div>


<p>and save. The mini-buffer in Emacs reads as <code>(Scala [ENSIME: (Disconnected)])</code>. We have to connect it by <code>M-x ensime</code> and press <code>ENTER</code> in Emacs. If everything works well, you should see in the mini-buffer </p>
<div class="highlight"><pre>    ENSIME ready. May the source be with you.
</pre></div>


<p>If we simply <code>C-c C-b s</code>, it will bring up the sbt console. Run project by typing 'run' in the sbt console. You will see the output message</p>
<div class="highlight"><pre>    Running sbt
    [info] Loading global plugins from /home/trgao10/.sbt/0.13/plugins
    [info] Set current project to hello_scala (in build file:/home/trgao10/Work/Scala/webGraphInfer/)
    &gt; run
    [info] Compiling 1 Scala source to /home/trgao10/Work/Scala/webGraphInfer/target/scala-2.10/classes...
    [info] Running greeter.Hello 
    Hello World
    [success] Total time: 3 s, completed Sep 30, 2015 10:54:33 PM
    &gt;
</pre></div>


<p>This indicates a successful setup.</p>
<hr />
<h2 id="compiling-spark-from-source">Compiling Spark from Source</h2>
<p>Compiling software libraries is often a huge pain &mdash; it often requires domain knowledge before you step into a domain. Compiling <code>Spark</code> is however an important part of developing applications with <code>Spark</code> since it evolves aggressively with new version poping up every a couple of months or so. Here we demonstrate how to compile the most recent version <code>1.5.1</code> (released on Oct 02, 2015) on a Ubuntu Linux laptop with amd64 architecture. Of course the official <a href="http://spark.apache.org/docs/latest/building-spark.html">"Building Spark"</a> page is the definitive starting point.</p>
<p>First of all, download the source code, untar, and <code>cd</code> into the decompressed folder. Make sure you have installed <code>maven</code>:</p>
<div class="highlight"><pre>sudo apt-get install maven
</pre></div>


<p>Temporarily increase the memory budget for <code>maven</code> (if you are using <code>fish</code> terminal instead of <code>bash</code>, you have to switch to <code>bash</code> before invoking <code>export</code>):</p>
<div class="highlight"><pre>export MAVEN_OPTS=&quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&quot;
</pre></div>


<p>Start compiling. Catcha: don't prepend your commands with <code>sudo</code>!</p>
<div class="highlight"><pre>mvn -DskipTests clean package
</pre></div>


<p>If you would like to build against certain version of hadoop (e.g., 2.4.0), try the following:</p>
<div class="highlight"><pre>mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package
</pre></div>


<p>The particular issue with <code>Spark 1.5.1</code> is that it requires <code>maven 3.3.3</code>, while only <code>3.0.5</code> is available in the official Ubuntu repository. Let's <code>purge</code> the old version and replace it with the more up-to-date one. First, remove the old stuff:</p>
<div class="highlight"><pre>sudo apt-get purge -y maven
</pre></div>


<p>Then we follow the steps in <a href="http://basicgroundwork.blogspot.com/2015/05/installing-maven-333-on-ubuntu-1504.html">this link</a>:</p>
<ol>
<li>
<p>Download Apache Maven 3.3.3 binary from repository using the following command</p>
<div class="highlight"><pre>wget http://mirrors.sonic.net/apache/maven/maven-3/3.3.3/binaries/apache-maven-3.3.3-bin.tar.gz
</pre></div>


</li>
<li>
<p>Unzip the binary with tar</p>
<div class="highlight"><pre>tar -zxf apache-maven-3.3.3-bin.tar.gz
</pre></div>


</li>
<li>
<p>Move the application directory to <code>/usr/local</code></p>
<div class="highlight"><pre>sudo cp -R apache-maven-3.3.3 /usr/local
</pre></div>


</li>
<li>
<p>Make a soft link in /usr/bin for universal access of mvn</p>
<div class="highlight"><pre>sudo ln -s /usr/local/apache-maven-3.3.3/bin/mvn /usr/bin/mvn
</pre></div>


</li>
<li>
<p>Verifify mvn installation</p>
<div class="highlight"><pre>mvn --version
</pre></div>


</li>
</ol>
<p>You should see the following printout:</p>
<div class="highlight"><pre>Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T07:57:37-04:00)
Maven home: /usr/local/apache-maven-3.3.3
Java version: 1.8.0_60, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-8-oracle/jre
Default locale: en_US, platform encoding: UTF-8
OS name: &quot;linux&quot;, version: &quot;3.13.0-65-generic&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;
</pre></div>


<p>After successfully compiling <code>Spark</code>, move it to <code>/usr/local/</code>:</p>
<div class="highlight"><pre>sudo mv ./spark-1.5.1 /usr/local/
</pre></div>


<p>It is most convenient if we build a soft link and only update the link when a newer version of <code>Spark</code> is installed. Installing a new <code>Spark</code> version is as simple as</p>
<div class="highlight"><pre>sudo rm /usr/local/share/spark
sudo ln -s /usr/local/spark-1.5.1 /usr/local/share/spark
</pre></div>


<p>Check the version is already updated:</p>
<div class="highlight"><pre>trgao10@Terranius:$ spark-shell
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark&#39;s repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel(&quot;INFO&quot;)
Welcome to
____              __
/ __/__  ___ _____/ /__
_\ \/ _ \/ _ `/ __/  &#39;_/
/___/ .__/\_,_/_/ /_/\_\   version 1.5.1
/_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_80)
Type in expressions to have them evaluated.
Type :help for more information.
15/10/08 00:33:56 WARN Utils: Your hostname, Terranius resolves to a loopback address: 127.0.1.1; using 192.168.2.14 instead (on interface wlan0)
15/10/08 00:33:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
15/10/08 00:33:57 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
Spark context available as sc.
SQL context available as sqlContext.
</pre></div>


<hr />
<h2 id="using-graphx-to-build-a-bipartite-graph">Using GraphX to Build a Bipartite Graph</h2>
<!-- To speed up the development process, we rely first on the spark-shell. -->

<!--     /usr/local/share/spark/bin/spark-shell  -->

<!-- In the loaded spark-shell, do some imports. -->

<!--     import org.apache.spark.sql.Row -->

<!--     import org.apache.spark.sql.types.{StructType,StructField,StringType} -->

<!-- Load in raw data and parse into DataFrames. -->

<!--     val rawData = sc.textFile("/home/trgao10/Work/Scala/SparkScalaGraph/data/websites_clean.csv").map( line=> { -->

<!--         Row.fromSeq(line.split(", ")) -->

<!--       }); -->

<!--     val schemaString = "dtbd_id,domain_name,ip_address,traffic_rank,class_ecomm,date_found,isp_name,admin_email,dns,enforce_status,registrant,registrar".split(","); -->

<!--     val schema = StructType(schemaString.map(fieldName => StructField(fieldName, StringType, false))); -->

<!--     val wsDataFrame = sqlContext.createDataFrame(rawData, schema) -->

<!-- View the registered schema: -->

<!--     wsDataFrame.printSchema -->

<!-- You will see the following schema printed out: -->

<!--     scala> wsDataFrame.printSchema -->

<!--     root -->

<!--      |-- dtbd_id: string (nullable = false) -->

<!--      |-- domain_name: string (nullable = false) -->

<!--      |-- ip_address: string (nullable = false) -->

<!--      |-- traffic_rank: string (nullable = false) -->

<!--      |-- class_ecomm: string (nullable = false) -->

<!--      |-- date_found: string (nullable = false) -->

<!--      |-- isp_name: string (nullable = false) -->

<!--      |-- admin_email: string (nullable = false) -->

<!--      |-- dns: string (nullable = false) -->

<!--      |-- enforce_status: string (nullable = false) -->

<!--      |-- registrant: string (nullable = false) -->

<!--      |-- registrar: string (nullable = false) -->

<!-- If you prefer, you can register the DataFrame as a table: -->

<!--     wsDataFrame.registerTempTable("wsData") -->

<!-- Try query: -->

<!--     scala> val result = sqlContext.sql("SELECT date_found FROM wsData") -->

<!--     result: org.apache.spark.sql.DataFrame = [date_found: string] -->

<!-- It ends up with strange errors. Have to deviate from this approach.... -->

<p>Like many popular graph processing libraries, GraphX represents a graph as a <em>property graph</em>. This is a class defined as</p>
<div class="highlight"><pre>    class Graph[VD, ED] {
        val vertices: VertexRDD[VD]
        val edges: EdgeRDD[ED,VD]
    }
</pre></div>


<p><code>VD</code> and <code>ED</code> here are Scala-type parameters of the classes <code>VertexRDD</code>, <code>EdgeRDD</code>, and <code>Graph</code>. These type parameters can be primitive types such as <code>String</code> or <code>Int</code>, but they can also be user-defined classes. See the following schematic definition of <code>VertexRDD</code> (from the <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">offial Spark documentation</a>).</p>
<div class="highlight"><pre>    class VertexRDD[VD] extends RDD[(VertexID, VD)] {
    // Filter the vertex set but preserves the internal index
    def filter(pred: Tuple2[VertexId, VD] =&gt; Boolean): VertexRDD[VD]
    // Transform the values without changing the ids (preserves the internal index)
    def mapValues[VD2](map: VD =&gt; VD2): VertexRDD[VD2]
    def mapValues[VD2](map: (VertexId, VD) =&gt; VD2): VertexRDD[VD2]
    // Show only vertices unique to this set based on their VertexId&#39;s
    def minus(other: RDD[(VertexId, VD)])
    // Remove vertices from this set that appear in the other set
    def diff(other: VertexRDD[VD]): VertexRDD[VD]
    // Join operators that take advantage of the internal indexing to accelerate joins (substantially)
    def leftJoin[VD2, VD3](other: RDD[(VertexId, VD2)])(f: (VertexId, VD, Option[VD2]) =&gt; VD3): VertexRDD[VD3]
    def innerJoin[U, VD2](other: RDD[(VertexId, U)])(f: (VertexId, VD, U) =&gt; VD2): VertexRDD[VD2]
    // Use the index on this RDD to accelerate a `reduceByKey` operation on the input RDD.
    def aggregateUsingIndex[VD2](other: RDD[(VertexId, VD2)], reduceFunc: (VD2, VD2) =&gt; VD2): VertexRDD[VD2]
    }
</pre></div>


<p>The <code>EdgeRDD</code> object takes one type parameter <code>ED</code>, and is actually a subclass that extends <code>RDD[Edge[ED]]</code>.</p>
<p>For the website data, each entry consists of the following properties:</p>
<ul>
<li>dtbd_id</li>
<li>domain_name</li>
<li>ip_address</li>
<li>traffic_rank</li>
<li>class_ecomm</li>
<li>date_found</li>
<li>isp_name</li>
<li>admin_email</li>
<li>dns</li>
<li>enforce_status</li>
<li>registrant</li>
<li>registrar</li>
</ul>
<p>We intend to model the website data as a <em>bipartite graph</em>. There will be two different types of vertices in this graph: "Domain" and "Info". Since <code>GraphX</code> only allows for a unified vertex type in a graph, we have to build an abstract vertex type and derive from it the "Domian Vertex" and "Info Vertex" subclasses. Defining a class in <code>scala</code> is easy:</p>
<div class="highlight"><pre>class VertexProperty extends Serializable {}
</pre></div>


<p>It is crucial that this class has to extend the "Serializable" object, for otherwise <code>Spark</code> will complain about missing class constructors when it comes to serializing the derived subclasses.</p>
<p>Subclass <code>Domain</code>:</p>
<div class="highlight"><pre>case class Domain(val dtbd_id: String, val class_ecomm: String, val date_found: String, val isp_name: String, val enforce_status: String, val registrar: String, val traffic_rank: String) extends VertexProperty;
</pre></div>


<p>Subclass <code>Info</code>:</p>
<div class="highlight"><pre>case class Info(val info: String, val category: String) extends VertexProperty;
</pre></div>


<p>Let's start building the graph. First we extract all domain vertices, one for each line in the raw data:</p>
<div class="highlight"><pre>val domainRDD: RDD[(VertexId, VertexProperty)] = rawDataWithIndex.map { line =&gt;
  val ID = line._2
  val DomainInfo = line._1
  (ID, Domain(DomainInfo(schemaIndexMap(&quot;dtbd_id&quot;)), DomainInfo(schemaIndexMap(&quot;class_ecomm&quot;)), DomainInfo(schemaIndexMap(&quot;date_found&quot;)), DomainInfo(schemaIndexMap(&quot;isp_name&quot;)), DomainInfo(schemaIndexMap(&quot;enforce_status&quot;)), DomainInfo(schemaIndexMap(&quot;registrar&quot;)), DomainInfo(schemaIndexMap(&quot;traffic_rank&quot;))))
};
</pre></div>


<p><code>schemaIndexMap</code> is a <code>Map</code> in <code>scala</code> that hashes each schema field to its position in the raw data record. The only reason it appears here is because I did not find a <code>Pandas</code>-like library in <code>scala</code> ....</p>
<div class="highlight"><pre>val schemaArray = &quot;dtbd_id,domain_name,ip_address,traffic_rank,class_ecomm,date_found,isp_name,admin_email,dns,enforce_status,registrant,registrar&quot;.split(&quot;,&quot;);
val schemaIndexMap = Map(schemaArray.zip((0 to schemaArray.length-1)).toArray: _*);
</pre></div>


<p>Constructing all <code>Info</code> vertices requires going through the raw data again. In order to save spatial complexity, we create <code>Info</code> vertices only for non-zero info fields.</p>
<div class="highlight"><pre>case class Info(val info: String, val category: String) extends VertexProperty;
val infoCatMap = Map(&quot;domain&quot;-&gt;&quot;domain_name&quot;, &quot;ip&quot;-&gt;&quot;ip_address&quot;, &quot;email&quot;-&gt;&quot;admin_email&quot;, &quot;dns&quot;-&gt;&quot;dns&quot;, &quot;name&quot;-&gt;&quot;registrant&quot;);
def peekInfo (data: Array[String], infoType: String) : String = {
  data(schemaIndexMap(infoCatMap(infoType)))
}
def makeInfo (data: Array[String], infoType: String) : Info = {
  val info = peekInfo(data, infoType)
  if (info == &quot;&quot;)
    return null
  else
    return Info(info, infoType)
}

val infoRDD: RDD[(VertexId, VertexProperty)] = rawDataWithIndex.flatMap { line =&gt;
  val data = line._1
  var infoArray = Array[Info]()
  if (peekInfo(data, &quot;domain&quot;) != &quot;&quot;)
    infoArray = infoArray :+ makeInfo(data, &quot;domain&quot;)
  if (peekInfo(data, &quot;ip&quot;) != &quot;&quot;)
    infoArray = infoArray :+ makeInfo(data, &quot;ip&quot;)
  if (peekInfo(data, &quot;email&quot;) != &quot;&quot;)
    infoArray = infoArray :+ makeInfo(data, &quot;email&quot;)
  if (peekInfo(data, &quot;dns&quot;) != &quot;&quot;)
    infoArray = infoArray :+ makeInfo(data, &quot;dns&quot;)
  if (peekInfo(data, &quot;name&quot;) != &quot;&quot;)
    infoArray = infoArray :+ makeInfo(data, &quot;name&quot;)
  infoArray
}.distinct().zipWithIndex.map(line =&gt; (line._2+dataSize, line._1));
</pre></div>


<p>Now we have to build edges. They are initiated with the syntax</p>
<div class="highlight"><pre>Edge(sourceIdx, targetIdx, edgeProperty)
</pre></div>


<p>So we need the source and target indices. The source indices are easy to find: they are simply the indices of the records in <code>domainRDD</code> (that's why we started with <code>rawDataWithIndex</code>). The <code>Map</code> object <code>infoIndexMap</code> is used for quickly referencing the index of the <code>Info</code> vertices; note that <code>Info</code> vertices should be indexed after <code>Domain</code> vertices &mdash; this is why we applied <code>zipWithIndex</code> to the raw data, instead of <code>zipWithUniqueId</code> which is much faster.</p>
<div class="highlight"><pre>val infoIndexMap = sc.broadcast(infoRDD.map { line =&gt;
  ((line._2).asInstanceOf[Info].info, line._1)
}.collectAsMap());
</pre></div>


<p>A key subtlty in building <code>infoIndexMap</code> is that we have to cast a <code>VertexProperty</code> class into an instance of <code>Info</code> subclass. For a discussion on casting types in <code>scala</code>, see <a href="http://stackoverflow.com/questions/931463/how-do-i-cast-a-variable-in-scala">this post</a>. Also note that we wrapped <code>infoIndexMap</code> into a <code>broadcast</code> variable in <code>scala</code>.</p>
<p>The links can now be easily built:</p>
<div class="highlight"><pre>val linkRDD: RDD[Edge[String]] = rawDataWithIndex.flatMap { line =&gt;
  val data = line._1
  var edgeArray = Array[Edge[String]]()
  if (peekInfo(data, &quot;domain&quot;) != &quot;&quot;)
    edgeArray = edgeArray :+ Edge(line._2, infoIndexMap.value(peekInfo(data, &quot;domain&quot;)), &quot;&quot;)
  if (peekInfo(data, &quot;ip&quot;) != &quot;&quot;)
    edgeArray = edgeArray :+ Edge(line._2, infoIndexMap.value(peekInfo(data, &quot;ip&quot;)), &quot;&quot;)
  if (peekInfo(data, &quot;email&quot;) != &quot;&quot;)
    edgeArray = edgeArray :+ Edge(line._2, infoIndexMap.value(peekInfo(data, &quot;email&quot;)), &quot;&quot;)
  if (peekInfo(data, &quot;dns&quot;) != &quot;&quot;)
    edgeArray = edgeArray :+ Edge(line._2, infoIndexMap.value(peekInfo(data, &quot;dns&quot;)), &quot;&quot;)
  if (peekInfo(data, &quot;name&quot;) != &quot;&quot;)
    edgeArray = edgeArray :+ Edge(line._2, infoIndexMap.value(peekInfo(data, &quot;name&quot;)), &quot;&quot;)
  edgeArray
}
</pre></div>


<p>With all these preparations, building a graph and extracting its connected components in <code>GraphX</code> is a piece of cake.</p>
<div class="highlight"><pre>val mmBG: Graph[VertexProperty, String] = Graph(domainRDD.union(infoRDD), linkRDD);
val ccMMBG = mmBG.connectedComponents();
</pre></div>


<p>The <code>ccMMBG</code> object is a new graph whose indices are all of the type <code>(VertexId, VertexId)</code>, in which the second <code>VertexId</code> is the smallest vertex id of all vertices in the same connected component as the vertex with the first <code>VertexId</code>. In the end of the code we print out some simple statistics for the connected components of the constructed bipartite graph.</p>
<div class="highlight"><pre>println(&quot;Total number of edges in the graph: &quot; + mmBG.edges.count);
println(&quot;Total number of vertices in the graph: &quot; + mmBG.vertices.count);
val ccNumVertices = 
  (ccMMBG.vertices.map(pair =&gt; (pair._2,1))
  .reduceByKey(_+_)  // count the number of vertices contained in each connected component (indexed by the smallest vertex index in the connecte dcomponent)
  .map(pair =&gt; pair._2)) // only maintain the number of vertices counted
println(&quot;Number of Connected Components: &quot; + ccNumVertices.count);
ListMap(ccNumVertices.countByValue().toSeq.sortBy(_._1):_*).foreach(line =&gt; println(line._2 + &quot; connected component(s) with &quot; + line._1 + &quot; vertices&quot;));
</pre></div>


<p>The last line in this print section involves sorting a <code>Map</code> object in <code>scala</code>. For more discussions about this topic, check <a href="https://www.safaribooksonline.com/library/view/scala-cookbook/9781449340292/ch11s23.html">this link</a> to an online verison of "Scala Cookbook", which might be of great help in itself.</p>
<p>Here is the complete <code>Main.scala</code>:</p>
<div class="highlight"><pre>    package sparkGraph

    import org.apache.spark._
    import org.apache.spark.graphx._
    import org.apache.spark.rdd.RDD
    import scala.collection.immutable.ListMap
    import org.apache.spark.SparkContext
    import org.apache.spark.SparkContext._
    import org.apache.spark.SparkConf
    import org.apache.log4j.Level
    import org.apache.log4j.Logger

    object sparkGraph {
      def main (args: Array[String]) {

        Logger.getLogger(&quot;org&quot;).setLevel(Level.OFF);
        Logger.getLogger(&quot;akka&quot;).setLevel(Level.OFF);

        val conf = new SparkConf().setAppName(&quot;SparkGraph&quot;).setMaster(&quot;local[*]&quot;);
        val sc = new SparkContext(conf);

        val schemaArray = &quot;dtbd_id,domain_name,ip_address,traffic_rank,class_ecomm,date_found,isp_name,admin_email,dns,enforce_status,registrant,registrar&quot;.split(&quot;,&quot;);
        val schemaIndexMap = Map(schemaArray.zip((0 to schemaArray.length-1)).toArray: _*);

        val numFields = schemaArray.length;

        val rawDataWithIndex = (sc.textFile(&quot;/home/trgao10/Work/Scala/SparkScalaGraph/data/websites_clean_small.csv&quot;)
          .map(_.split(&quot;,\t&quot;))
          .filter(_.length == numFields) // TODO: FIX THIS AD-HOC DATA PROCESSING
          .zipWithIndex);

        val dataSize = rawDataWithIndex.count();

        class VertexProperty extends Serializable {}

        case class Domain(val dtbd_id: String, val class_ecomm: String, val date_found: String, val isp_name: String, val enforce_status: String, val registrar: String, val traffic_rank: String) extends VertexProperty;

        val domainRDD: RDD[(VertexId, VertexProperty)] = rawDataWithIndex.map { line =&gt;
          val ID = line._2
          val DomainInfo = line._1
          (ID, Domain(DomainInfo(schemaIndexMap(&quot;dtbd_id&quot;)), DomainInfo(schemaIndexMap(&quot;class_ecomm&quot;)), DomainInfo(schemaIndexMap(&quot;date_found&quot;)), DomainInfo(schemaIndexMap(&quot;isp_name&quot;)), DomainInfo(schemaIndexMap(&quot;enforce_status&quot;)), DomainInfo(schemaIndexMap(&quot;registrar&quot;)), DomainInfo(schemaIndexMap(&quot;traffic_rank&quot;))))
        };

        case class Info(val info: String, val category: String) extends VertexProperty;
        val infoCatMap = Map(&quot;domain&quot;-&gt;&quot;domain_name&quot;, &quot;ip&quot;-&gt;&quot;ip_address&quot;, &quot;email&quot;-&gt;&quot;admin_email&quot;, &quot;dns&quot;-&gt;&quot;dns&quot;, &quot;name&quot;-&gt;&quot;registrant&quot;);
        def peekInfo (data: Array[String], infoType: String) : String = {
          data(schemaIndexMap(infoCatMap(infoType)))
        }
        def makeInfo (data: Array[String], infoType: String) : Info = {
          val info = peekInfo(data, infoType)
          if (info == &quot;&quot;)
            return null
          else
            return Info(info, infoType)
        }

        val infoRDD: RDD[(VertexId, VertexProperty)] = rawDataWithIndex.flatMap { line =&gt;
          val data = line._1
          var infoArray = Array[Info]()
          if (peekInfo(data, &quot;domain&quot;) != &quot;&quot;)
            infoArray = infoArray :+ makeInfo(data, &quot;domain&quot;)
          if (peekInfo(data, &quot;ip&quot;) != &quot;&quot;)
            infoArray = infoArray :+ makeInfo(data, &quot;ip&quot;)
          if (peekInfo(data, &quot;email&quot;) != &quot;&quot;)
            infoArray = infoArray :+ makeInfo(data, &quot;email&quot;)
          if (peekInfo(data, &quot;dns&quot;) != &quot;&quot;)
            infoArray = infoArray :+ makeInfo(data, &quot;dns&quot;)
          if (peekInfo(data, &quot;name&quot;) != &quot;&quot;)
            infoArray = infoArray :+ makeInfo(data, &quot;name&quot;)
          infoArray
        }.distinct().zipWithIndex.map(line =&gt; (line._2+dataSize, line._1));

        val infoIndexMap = sc.broadcast(infoRDD.map { line =&gt;
          ((line._2).asInstanceOf[Info].info, line._1)
        }.collectAsMap());

        val linkRDD: RDD[Edge[String]] = rawDataWithIndex.flatMap { line =&gt;
          val data = line._1
          var edgeArray = Array[Edge[String]]()
          if (peekInfo(data, &quot;domain&quot;) != &quot;&quot;)
            edgeArray = edgeArray :+ Edge(line._2, infoIndexMap.value(peekInfo(data, &quot;domain&quot;)), &quot;&quot;)
          if (peekInfo(data, &quot;ip&quot;) != &quot;&quot;)
            edgeArray = edgeArray :+ Edge(line._2, infoIndexMap.value(peekInfo(data, &quot;ip&quot;)), &quot;&quot;)
          if (peekInfo(data, &quot;email&quot;) != &quot;&quot;)
            edgeArray = edgeArray :+ Edge(line._2, infoIndexMap.value(peekInfo(data, &quot;email&quot;)), &quot;&quot;)
          if (peekInfo(data, &quot;dns&quot;) != &quot;&quot;)
            edgeArray = edgeArray :+ Edge(line._2, infoIndexMap.value(peekInfo(data, &quot;dns&quot;)), &quot;&quot;)
          if (peekInfo(data, &quot;name&quot;) != &quot;&quot;)
            edgeArray = edgeArray :+ Edge(line._2, infoIndexMap.value(peekInfo(data, &quot;name&quot;)), &quot;&quot;)
          edgeArray
        }

        val mmBG: Graph[VertexProperty, String] = Graph(domainRDD.union(infoRDD), linkRDD);
        val ccMMBG = mmBG.connectedComponents();

        println(&quot;Total number of edges in the graph: &quot; + mmBG.edges.count);
        println(&quot;Total number of vertices in the graph: &quot; + mmBG.vertices.count);
        val ccNumVertices = 
          (ccMMBG.vertices.map(pair =&gt; (pair._2,1))
          .reduceByKey(_+_)  // count the number of vertices contained in each connected component (indexed by the smallest vertex index in the connecte dcomponent)
          .map(pair =&gt; pair._2)) // only maintain the number of vertices counted
        println(&quot;Number of Connected Components: &quot; + ccNumVertices.count);

        // ccMMBG.vertices consists of (VertexId, VertexId) pairs, in which the second VertexId represents the smallest ID of the vertex in the same connected component
        ListMap(ccNumVertices.countByValue().toSeq.sortBy(_._1):_*).foreach(line =&gt; println(line._2 + &quot; connected component(s) with &quot; + line._1 + &quot; vertices&quot;));
      }
    }
</pre></div>


<p>Finally, we are ready to compile and run the code. With a project folder and <code>build.sbt</code> structured as in the section "Set up Emacs+Ensime", we can use <code>sbt</code> in the following manner:</p>
<div class="highlight"><pre>trgao10@Terranius ~/W/S/webGraphInfer&gt; sbt
[info] Loading global plugins from /home/trgao10/.sbt/0.13/plugins
[info] Set current project to scala_graphx (in build file:/home/trgao10/Work/Scala/webGraphInfer/)
&gt; compile
[info] Compiling 1 Scala source to /home/trgao10/Work/Scala/webGraphInfer/target/scala-2.10/classes...
[success] Total time: 15 s, completed Oct 7, 2015 11:12:23 PM
&gt; run
</pre></div>


<p>We are able to process a bipartite graph with 800,000 vertices in just 49 seconds. See terminal outputs below. The largest connected component contains 266,475 vertices.</p>
<div class="highlight"><pre>[info] Running sparkGraph.sparkGraph 
Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties
15/10/07 23:13:15 INFO Remoting: Starting remoting
15/10/07 23:13:16 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.2.14:50443]
Total number of edges in the graph: 801189                                      
Total number of vertices in the graph: 289326
Number of Connected Components: 1426
125 connected component(s) with 1 vertices
31 connected component(s) with 2 vertices
134 connected component(s) with 3 vertices
47 connected component(s) with 4 vertices
36 connected component(s) with 5 vertices
139 connected component(s) with 6 vertices
117 connected component(s) with 7 vertices
66 connected component(s) with 8 vertices
83 connected component(s) with 9 vertices
72 connected component(s) with 10 vertices
65 connected component(s) with 11 vertices
39 connected component(s) with 12 vertices
25 connected component(s) with 13 vertices
38 connected component(s) with 14 vertices
34 connected component(s) with 15 vertices
22 connected component(s) with 16 vertices
23 connected component(s) with 17 vertices
25 connected component(s) with 18 vertices
18 connected component(s) with 19 vertices
12 connected component(s) with 20 vertices
14 connected component(s) with 21 vertices
12 connected component(s) with 22 vertices
21 connected component(s) with 23 vertices
8 connected component(s) with 24 vertices
11 connected component(s) with 25 vertices
13 connected component(s) with 26 vertices
13 connected component(s) with 27 vertices
7 connected component(s) with 28 vertices
6 connected component(s) with 29 vertices
11 connected component(s) with 30 vertices
7 connected component(s) with 31 vertices
6 connected component(s) with 32 vertices
2 connected component(s) with 33 vertices
5 connected component(s) with 34 vertices
6 connected component(s) with 35 vertices
5 connected component(s) with 36 vertices
4 connected component(s) with 37 vertices
4 connected component(s) with 38 vertices
5 connected component(s) with 39 vertices
4 connected component(s) with 40 vertices
2 connected component(s) with 41 vertices
2 connected component(s) with 42 vertices
4 connected component(s) with 43 vertices
4 connected component(s) with 44 vertices
6 connected component(s) with 45 vertices
1 connected component(s) with 46 vertices
5 connected component(s) with 47 vertices
3 connected component(s) with 48 vertices
3 connected component(s) with 49 vertices
2 connected component(s) with 50 vertices
2 connected component(s) with 51 vertices
1 connected component(s) with 52 vertices
3 connected component(s) with 53 vertices
2 connected component(s) with 54 vertices
3 connected component(s) with 55 vertices
2 connected component(s) with 56 vertices
2 connected component(s) with 57 vertices
1 connected component(s) with 59 vertices
1 connected component(s) with 60 vertices
1 connected component(s) with 61 vertices
1 connected component(s) with 62 vertices
2 connected component(s) with 63 vertices
1 connected component(s) with 64 vertices
2 connected component(s) with 66 vertices
1 connected component(s) with 67 vertices
3 connected component(s) with 68 vertices
3 connected component(s) with 69 vertices
2 connected component(s) with 70 vertices
1 connected component(s) with 71 vertices
2 connected component(s) with 72 vertices
1 connected component(s) with 73 vertices
1 connected component(s) with 74 vertices
1 connected component(s) with 77 vertices
1 connected component(s) with 78 vertices
2 connected component(s) with 81 vertices
1 connected component(s) with 84 vertices
1 connected component(s) with 86 vertices
2 connected component(s) with 88 vertices
1 connected component(s) with 92 vertices
1 connected component(s) with 94 vertices
1 connected component(s) with 97 vertices
1 connected component(s) with 98 vertices
1 connected component(s) with 99 vertices
2 connected component(s) with 104 vertices
1 connected component(s) with 107 vertices
1 connected component(s) with 110 vertices
1 connected component(s) with 113 vertices
1 connected component(s) with 115 vertices
1 connected component(s) with 118 vertices
1 connected component(s) with 121 vertices
1 connected component(s) with 124 vertices
1 connected component(s) with 125 vertices
1 connected component(s) with 129 vertices
1 connected component(s) with 130 vertices
1 connected component(s) with 135 vertices
1 connected component(s) with 136 vertices
1 connected component(s) with 149 vertices
1 connected component(s) with 150 vertices
1 connected component(s) with 153 vertices
1 connected component(s) with 158 vertices
1 connected component(s) with 159 vertices
1 connected component(s) with 161 vertices
1 connected component(s) with 181 vertices
1 connected component(s) with 213 vertices
1 connected component(s) with 230 vertices
1 connected component(s) with 256 vertices
1 connected component(s) with 284 vertices
1 connected component(s) with 301 vertices
1 connected component(s) with 313 vertices
1 connected component(s) with 266475 vertices
[success] Total time: 49 s, completed Oct 7, 2015 11:14:00 PM
</pre></div>


<p>To learn more about <code>sbt</code>, check its <a href="http://www.scala-sbt.org/0.13/tutorial/">official online tutorial</a>.</p>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/bglbp.html" rel="bookmark"
                           title="Permalink to Loopy Belief Propagation: A Toy Example">Loopy Belief Propagation: A Toy Example</a></h1>
                </header>

                <div class="entry-content">
                <!-- <footer class="post-info">
        <abbr class="published" title="2015-09-16T01:26:00-04:00">
                Published: Wed 16 September 2015
        </abbr>
		<br />
        <abbr class="modified" title="2015-09-16T01:30:00-04:00">
                Updated: Wed 16 September 2015
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/tingran-gao.html">Tingran Gao</a>
        </address>
<p>In <a href="/category/expository.html">Expository</a>.</p>
<p>tags: <a href="/tag/machine-learning.html">Machine Learning</a> <a href="/tag/probabilistic-graphical-model.html">Probabilistic Graphical Model</a> </p>
</footer><!-- /.post-info -->                
                <a class="readmore" href="/bglbp.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
    <section id="extras" class="body">
      <div class="blogroll">
        <h2>useful links</h2>
        <!--<h2>blogroll</h2>-->
        <ul>
          <li><a href="http://www.math.duke.edu/">Department of Mathematics</a></li>
          <li><a href="http://www.math.duke.edu/courses/math_everywhere/">Math Everywhere @ Duke</a></li>
          <li><a href="http://www.wolframalpha.com/">Wolfram Alpha</a></li>
        </ul>
      </div><!-- /.blogroll -->
      <div class="social">
        <h2>social</h2>
        <ul>

          <li><a href="https://github.com/trgao10">GitHub</a></li>
          <li><a href="https://www.linkedin.com/pub/tingran-gao/89/8a8/a8a">LinkedIn</a></li>
        </ul>
      </div><!-- /.social -->
    </section><!-- /#extras -->

    <footer id="contentinfo" class="body">
      <!--<address id="about" class="vcard body">
        Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
      </address>--><!-- /#about -->
      <p>Powered by <a href="http://getpelican.com/">Pelican</a><br>last modified:
        <script>document.write(document.lastModified);</script>
      </p>
      <!--<p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>-->
    </footer><!-- /#contentinfo -->

    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-67249529-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = 'https://ssl.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
  </body>
</html>